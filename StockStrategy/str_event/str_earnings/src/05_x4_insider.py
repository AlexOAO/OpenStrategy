#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éšæ®µ5ï¼šX4è¨ˆç®—ï¼ˆåå¤§è‚¡æ±æŒè‚¡è®ŠåŒ–ç‡ï¼‰èˆ‡æ•£æˆ¶æŒè‚¡åˆ†å¸ƒ
ä»¥ABSTN1å–å¾—åå¤§è‚¡æ±ï¼ˆä¸å«è‘£ç›£ï¼‰æŒè‚¡è®ŠåŒ–ç‡ï¼Œä¸¦ä»¥ADCSHRè¨ˆç®—æ•£æˆ¶é›†ä¸­åº¦
"""

import pandas as pd
from datetime import timedelta
import subprocess
from pathlib import Path
from typing import Optional
from concurrent.futures import ThreadPoolExecutor, as_completed


def get_project_root():
    current = Path.cwd()
    return current.parent if current.name == 'src' else current

PROJECT_ROOT = get_project_root()


class InsiderTradingCalculator:
    def __init__(
        self,
        event_list_path: Optional[Path] = None,
        car_data_path: Optional[Path] = None,
        tool_abstn1: Optional[str] = None,
        tool_adcshr: Optional[str] = None,
        lookback_days: int = 365,  # ğŸš€ å„ªåŒ–ï¼šå¢åŠ å¿«å–è¦–çª—è‡³365å¤©
    ):
        self.event_list_path = event_list_path or (PROJECT_ROOT / 'data/processed/event_list.csv')
        self.car_data_path = car_data_path or (PROJECT_ROOT / 'data/processed/car_data.csv')
        self.tool_abstn1 = tool_abstn1 or str(PROJECT_ROOT / 'tej_tool_TWN_ABSTN1.py')
        self.tool_adcshr = tool_adcshr or str(PROJECT_ROOT / 'tej_tool_TWN_ADCSHR.py')
        self.lookback_days = lookback_days
        self.events_df = None
        self.x4_results = []
        self.abstn1_output_dir = PROJECT_ROOT / 'output_abstn1'
        self.adcshr_output_dir = PROJECT_ROOT / 'output_adcshr'

    def load_events(self):
        """
        è¼‰å…¥äº‹ä»¶åˆ—è¡¨ï¼Œå„ªå…ˆä½¿ç”¨ car_data.csvï¼ˆåªè™•ç†æˆåŠŸè¨ˆç®—CARçš„äº‹ä»¶ï¼‰
        å¦‚æœ car_data.csv ä¸å­˜åœ¨ï¼Œå‰‡ä½¿ç”¨ event_list.csv
        """
        if self.car_data_path.exists():
            print(f"å¾ CAR è³‡æ–™è¼‰å…¥äº‹ä»¶: {self.car_data_path}")
            self.events_df = pd.read_csv(self.car_data_path)
            # car_data.csv ä½¿ç”¨ event_date æ¬„ä½ï¼Œéœ€çµ±ä¸€ç‚º mdate
            if 'event_date' in self.events_df.columns:
                self.events_df['mdate'] = pd.to_datetime(self.events_df['event_date'])
            elif 'mdate' in self.events_df.columns:
                self.events_df['mdate'] = pd.to_datetime(self.events_df['mdate'])
            
            # åªä¿ç•™å¿…è¦æ¬„ä½
            self.events_df = self.events_df[['coid', 'mdate']].copy()
            print(f"è¼‰å…¥ {len(self.events_df)} ç­†æˆåŠŸè¨ˆç®—CARçš„äº‹ä»¶")
        else:
            print(f"CAR è³‡æ–™ä¸å­˜åœ¨ï¼Œå¾äº‹ä»¶åˆ—è¡¨è¼‰å…¥: {self.event_list_path}")
            self.events_df = pd.read_csv(self.event_list_path)
            self.events_df['mdate'] = pd.to_datetime(self.events_df['mdate'])
            print(f"è¼‰å…¥ {len(self.events_df)} ç­†äº‹ä»¶")
        
        return self

    def _ensure_abstn1_file(self, coid: str, event_date: pd.Timestamp) -> Optional[Path]:
        self.abstn1_output_dir.mkdir(exist_ok=True)
        start_date = (event_date - timedelta(days=self.lookback_days)).strftime('%Y-%m-%d')
        end_date = (event_date - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # æª¢æŸ¥å¤šç¨®å¯èƒ½çš„æª”åï¼ˆbasic æˆ– all ç¾¤çµ„ï¼‰
        possible_files = [
            self.abstn1_output_dir / f"abstn1_{coid}_{start_date.replace('-', '')}_{end_date.replace('-', '')}_all.csv",
            self.abstn1_output_dir / f"abstn1_{coid}_{start_date.replace('-', '')}_{end_date.replace('-', '')}_basic.csv",
            self.abstn1_output_dir / f"abstn1_{coid}_{start_date.replace('-', '')}_{end_date.replace('-', '')}_management.csv",
        ]
        
        # æª¢æŸ¥æ˜¯å¦å·²æœ‰å¯ç”¨æª”æ¡ˆ
        existing_file = None
        for file_path in possible_files:
            if file_path.exists():
                try:
                    df = pd.read_csv(file_path)
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“šä¸”æ—¥æœŸç¯„åœè¶³å¤ 
                    if len(df) > 0 and 'mdate' in df.columns:
                        df['mdate'] = pd.to_datetime(df['mdate'])
                        cache_start = df['mdate'].min()
                        cache_end = df['mdate'].max()
                        required_start = pd.Timestamp(start_date)
                        required_end = pd.Timestamp(end_date)
                        # cache ç¯„åœè¶³å¤ ä¸”åŒ…å«å¿…è¦æ¬„ä½
                        if cache_start <= required_start and cache_end >= required_end:
                            # æª¢æŸ¥æ˜¯å¦æœ‰ fld008 æ¬„ä½ï¼ˆåå¤§è‚¡æ±ï¼‰
                            if 'fld008' in df.columns:
                                existing_file = file_path
                                break
                except:
                    continue
        
        if existing_file:
            return existing_file
        
        # éœ€è¦é‡æ–°æŠ“å–ï¼Œä½¿ç”¨ all ç¾¤çµ„ç¢ºä¿åŒ…å«æ‰€æœ‰æ¬„ä½
        file_path = possible_files[0]  # ä½¿ç”¨ all ç¾¤çµ„çš„æª”å
        cmd = [
            'python3',
            self.tool_abstn1,
            '-s', str(coid),
            '--start-date', start_date,
            '--end-date', end_date,
            '-f', 'all',
        ]
        try:
            result = subprocess.run(cmd, cwd=str(PROJECT_ROOT), capture_output=True, timeout=120)
            if result.returncode != 0:
                print(f"  [ABSTN1] ä¸‹è¼‰å¤±æ•— {coid}: {result.stderr.decode(errors='ignore').strip()}")
        except Exception as exc:
            print(f"  [ABSTN1] å‘¼å«å¤±æ•— {coid}: {exc}")

        return file_path if file_path.exists() else None

    def _ensure_adcshr_file(self, coid: str, event_date: pd.Timestamp) -> Optional[Path]:
        self.adcshr_output_dir.mkdir(exist_ok=True)
        start_date = (event_date - timedelta(days=self.lookback_days)).strftime('%Y-%m-%d')
        end_date = (event_date - timedelta(days=1)).strftime('%Y-%m-%d')
        filename = f"ADCSHR_{coid}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv"
        file_path = self.adcshr_output_dir / filename

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        need_fetch = True
        if file_path.exists():
            try:
                df = pd.read_csv(file_path)
                # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“šä¸”æ—¥æœŸç¯„åœè¶³å¤ 
                if len(df) > 0 and 'mdate' in df.columns:
                    df['mdate'] = pd.to_datetime(df['mdate'])
                    cache_start = df['mdate'].min()
                    cache_end = df['mdate'].max()
                    required_start = pd.Timestamp(start_date)
                    required_end = pd.Timestamp(end_date)
                    # cache ç¯„åœè¶³å¤ å°±ä¸ç”¨é‡æŠ“
                    if cache_start <= required_start and cache_end >= required_end:
                        need_fetch = False
            except:
                need_fetch = True

        if need_fetch:
            cmd = [
                'python3',
                self.tool_adcshr,
                '--coid', str(coid),
                '--start-date', start_date,
                '--end-date', end_date,
            ]
            try:
                result = subprocess.run(cmd, cwd=str(PROJECT_ROOT), capture_output=True, timeout=120)
                if result.returncode != 0:
                    print(f"  [ADCSHR] ä¸‹è¼‰å¤±æ•— {coid}: {result.stderr.decode(errors='ignore').strip()}")
            except Exception as exc:
                print(f"  [ADCSHR] å‘¼å«å¤±æ•— {coid}: {exc}")

        return file_path if file_path.exists() else None

    @staticmethod
    def _calc_top10_change(df: pd.DataFrame, event_date: pd.Timestamp) -> Optional[float]:
        """
        è¨ˆç®—åå¤§è‚¡æ±ï¼ˆä¸å«è‘£ç›£ï¼‰æŒè‚¡è®ŠåŒ–ç‡ï¼ˆX4ï¼‰
        è®ŠåŒ–ç‡ = (æœŸæœ«æŒè‚¡ - æœŸåˆæŒè‚¡) / æœŸåˆæŒè‚¡
        """
        df = df.copy()
        df['mdate'] = pd.to_datetime(df['mdate'])
        t_minus_60 = event_date - timedelta(days=60)
        t_minus_1 = event_date - timedelta(days=1)

        df_t60 = df[df['mdate'] <= t_minus_60].tail(1)
        df_t1 = df[df['mdate'] <= t_minus_1].tail(1)

        if df_t60.empty or df_t1.empty:
            return None

        # fld008: åå¤§è‚¡æ±æŒè‚¡(ä¸å«è‘£ç›£) è‚¡æ•¸
        holding_field = 'fld008' if 'fld008' in df.columns else None
        if holding_field is None:
            return None

        holding_t60 = df_t60[holding_field].iloc[0]
        holding_t1 = df_t1[holding_field].iloc[0]
        
        # è¨ˆç®—è®ŠåŒ–ç‡ï¼š(æœŸæœ« - æœŸåˆ) / æœŸåˆ
        if pd.isna(holding_t60) or pd.isna(holding_t1) or holding_t60 == 0:
            return None
        
        change_rate = (holding_t1 - holding_t60) / holding_t60 * 100.0  # è½‰ç‚ºç™¾åˆ†æ¯”
        return change_rate

    @staticmethod
    def _calc_retail_ratio(df: pd.DataFrame, event_date: pd.Timestamp) -> Optional[float]:
        """
        è¨ˆç®—æ•£æˆ¶æŒè‚¡æ¯”ä¾‹è®ŠåŒ–ç‡ï¼ˆX6è®Šæ•¸ï¼‰

        æ•£æˆ¶å®šç¾©ï¼š400å¼µä»¥ä¸‹
        è®ŠåŒ–ç‡ = (ç•¶æœŸæ•£æˆ¶æŒè‚¡% - ä¸ŠæœŸæ•£æˆ¶æŒè‚¡%) / ä¸ŠæœŸæ•£æˆ¶æŒè‚¡%
        
        åŒ…å«ç´šè·ï¼š1å¼µä»¥ä¸‹(a), 1-5å¼µ(b), 5-10å¼µ(c), 10-15å¼µ(d), 15-20å¼µ(e),
                 20-30å¼µ(f), 30-40å¼µ(ga), 40-50å¼µ(gb), 50-100å¼µ(h),
                 100-200å¼µ(i), 200-400å¼µ(j)
        """
        df = df.copy()
        df['mdate'] = pd.to_datetime(df['mdate'])
        
        # T-1 (ç•¶æœŸ) å’Œ T-60 (ä¸ŠæœŸ)
        t_minus_1 = event_date - timedelta(days=1)
        t_minus_60 = event_date - timedelta(days=60)
        
        current = df[df['mdate'] <= t_minus_1].tail(1)
        previous = df[df['mdate'] <= t_minus_60].tail(1)
        
        if current.empty or previous.empty:
            return None

        # 400å¼µä»¥ä¸‹çš„æ‰€æœ‰ç´šè·ï¼ˆæŒè‚¡æ¯”ä¾‹æ¬„ä½ä»¥03çµå°¾ï¼‰
        required_cols = {'a03', 'b03', 'c03', 'd03', 'e03', 'f03',
                        'ga03', 'gb03', 'h03', 'i03', 'j03'}

        if not required_cols.issubset(current.columns) or not required_cols.issubset(previous.columns):
            return None

        # åŠ ç¸½400å¼µä»¥ä¸‹çš„æŒè‚¡æ¯”ä¾‹
        retail_ratio_current = float(current[list(required_cols)].iloc[0].sum())
        retail_ratio_previous = float(previous[list(required_cols)].iloc[0].sum())
        
        # è¨ˆç®—è®ŠåŒ–ç‡ï¼š(ç•¶æœŸ - ä¸ŠæœŸ) / ä¸ŠæœŸ
        if pd.isna(retail_ratio_previous) or pd.isna(retail_ratio_current) or retail_ratio_previous == 0:
            return None
        
        change_rate = (retail_ratio_current - retail_ratio_previous) / retail_ratio_previous * 100.0
        return change_rate

    def calculate_x4_for_event(self, coid, event_date):
        try:
            abstn1_file = self._ensure_abstn1_file(str(coid), event_date)
            adcshr_file = self._ensure_adcshr_file(str(coid), event_date)

            top10_change = None
            retail_ratio = None

            if abstn1_file is not None:
                df_abstn1 = pd.read_csv(abstn1_file)
                top10_change = self._calc_top10_change(df_abstn1, event_date)

            if adcshr_file is not None:
                df_adcshr = pd.read_csv(adcshr_file)
                retail_ratio = self._calc_retail_ratio(df_adcshr, event_date)

            # å³ä½¿æ•¸æ“šéƒ¨åˆ†ç¼ºå¤±ä¹Ÿä¿ç•™è¨˜éŒ„ï¼ˆç”¨ NaN è¡¨ç¤ºï¼‰
            # é€™æ¨£å¯ä»¥åœ¨å¾ŒçºŒåˆ†æä¸­è™•ç†ç¼ºå¤±å€¼
            return {
                'coid': coid,
                'event_date': event_date,
                'X4_top10_change_rate': top10_change,  # åå¤§è‚¡æ±æŒè‚¡è®ŠåŒ–ç‡
                'X6_retail_change_rate': retail_ratio,  # æ•£æˆ¶æŒè‚¡æ¯”ä¾‹è®ŠåŒ–ç‡ï¼ˆåŸX9æ”¹ç‚ºX6ï¼‰
            }

        except Exception as e:
            print(f"  [éšæ®µ5] éŒ¯èª¤ ({coid}): {str(e)}")
            return None

    def process_events(self, sample_size=None, start_date='2020-01-01', end_date='2025-12-31'):
        print("="*80)
        print("éšæ®µ5ï¼šX4ï¼ˆåå¤§è‚¡æ±æŒè‚¡è®ŠåŒ–ç‡ï¼‰èˆ‡X6ï¼ˆæ•£æˆ¶æŒè‚¡è®ŠåŒ–ç‡ï¼‰è¨ˆç®—")
        print("="*80)

        # ä½¿ç”¨åƒæ•¸åŒ–çš„æ—¥æœŸç¯„åœéæ¿¾
        start_ts = pd.Timestamp(start_date)
        end_ts = pd.Timestamp(end_date)
        
        events_filtered = self.events_df[
            (self.events_df['mdate'] >= start_ts) & 
            (self.events_df['mdate'] <= end_ts)
        ].copy()
        # çµ±ä¸€æ’åºï¼šå…ˆæŒ‰æ—¥æœŸã€å†æŒ‰è‚¡ç¥¨ä»£è™Ÿï¼ˆç¢ºä¿æ‰€æœ‰éšæ®µè™•ç†ç›¸åŒé †åºçš„äº‹ä»¶ï¼‰
        events_sorted = events_filtered.sort_values(['mdate', 'coid'], ascending=True)
        events_to_process = events_sorted.head(sample_size) if sample_size else events_sorted

        # çµ±è¨ˆè®Šæ•¸
        x4_success = 0  # X4 æˆåŠŸå–å¾—æ•¸é‡
        x6_success = 0  # X6 æˆåŠŸå–å¾—æ•¸é‡
        all_success = 0  # å…©è€…éƒ½æˆåŠŸçš„æ•¸é‡
        
        total = len(events_to_process)
        
        # ğŸš€ ä½¿ç”¨å¤šç·šç¨‹ä¸¦è¡Œè™•ç†ï¼ˆåŠ é€Ÿ 3-5 å€ï¼‰
        max_workers = 6  # å¢åŠ åˆ° 8 å€‹ç·šç¨‹ä»¥æå‡é€Ÿåº¦
        print(f"ğŸš€ ä½¿ç”¨ {max_workers} å€‹ç·šç¨‹ä¸¦è¡Œè™•ç†\n")

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»å‹™
            future_to_event = {}
            for idx, (_, row) in enumerate(events_to_process.iterrows(), 1):
                coid = row['coid']
                event_date = row['mdate']
                
                future = executor.submit(self.calculate_x4_for_event, coid, event_date)
                future_to_event[future] = (idx, coid, event_date)
            
            # æ”¶é›†çµæœï¼ˆæŒ‰å®Œæˆé †åºï¼‰
            completed = 0
            for future in as_completed(future_to_event):
                idx, coid, event_date = future_to_event[future]
                completed += 1
                
                try:
                    result = future.result()
                    if result:
                        self.x4_results.append(result)
                        
                        # æ›´æ–°çµ±è¨ˆ
                        has_x4 = result['X4_top10_change_rate'] is not None and \
                                 not pd.isna(result['X4_top10_change_rate'])
                        has_x6 = result['X6_retail_change_rate'] is not None and \
                                 not pd.isna(result['X6_retail_change_rate'])
                        
                        if has_x4:
                            x4_success += 1
                        if has_x6:
                            x6_success += 1
                        if has_x4 and has_x6:
                            all_success += 1
                        
                        # ç‹€æ…‹é¡¯ç¤º
                        status = []
                        if has_x4:
                            status.append("X4âœ“")
                        if has_x6:
                            status.append("X6âœ“")
                        
                        status_str = ', '.join(status) if status else 'ç„¡æ•¸æ“š'
                        print(f"âœ“ [{completed}/{total}] {coid} @ {event_date.strftime('%Y-%m-%d')} [{status_str}]")
                    else:
                        print(f"âœ— [{completed}/{total}] {coid} @ {event_date.strftime('%Y-%m-%d')} [è™•ç†å¤±æ•—]")
                except Exception as e:
                    print(f"âœ— [{completed}/{total}] {coid} @ {event_date.strftime('%Y-%m-%d')} - éŒ¯èª¤: {e}")

        # æœ€çµ‚çµ±è¨ˆ
        print(f"\n{'='*80}")
        print(f"éšæ®µ5å®Œæˆçµ±è¨ˆ:")
        print(f"  ç¸½è™•ç†äº‹ä»¶: {total}")
        print(f"  æˆåŠŸè¨˜éŒ„: {len(self.x4_results)}")
        print(f"  X4 (åå¤§è‚¡æ±è®ŠåŒ–ç‡) æˆåŠŸç‡: {x4_success}/{total} ({x4_success/total*100:.1f}%)")
        print(f"  X6 (æ•£æˆ¶è®ŠåŒ–ç‡) æˆåŠŸç‡: {x6_success}/{total} ({x6_success/total*100:.1f}%)")
        print(f"  å…©è€…çš†æœ‰: {all_success}/{total} ({all_success/total*100:.1f}%)")
        print(f"{'='*80}\n")

    def save_results(self, output_path=None):
        output_path = output_path or (PROJECT_ROOT / 'data/processed/x4_insider.csv')
        if not self.x4_results:
            print("ç„¡X4çµæœ")
            return
        df = pd.DataFrame(self.x4_results)
        df.to_csv(output_path, index=False)
        print(f"X4/X6å·²å„²å­˜: {output_path}")


def main():
    calculator = InsiderTradingCalculator()
    calculator.load_events()
    calculator.process_events(sample_size=35)
    calculator.save_results()


if __name__ == '__main__':
    main()
